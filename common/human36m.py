import os
from collections import defaultdict
import pickle

import numpy as np
import cv2

import torch
from torch.utils.data import Dataset

from common.camera import Camera, normalize_screen_coordinates, unnormalize_screen_coordinates
from common import multiview
from common.img import crop_keypoints_img, resize_keypoints_img


class Human36MMultiViewDataset(Dataset):
    """
        Human3.6M for multiview tasks.
    """
    def __init__(
            self,
            h36m_root='/Vol1/dbstore/datasets/Human3.6M/processed/',
            labels_path='/Vol1/dbstore/datasets/Human3.6M/extra/human36m-multiview-labels-SSDbboxes.npy',
            pred_results_path=None,
            pred_2d_results_path=None,
            pred_2d_error_dis_path=None,
            image_shape=(256, 256),
            heatmap_shape=(96, 96),
            train=False,
            test=False,
            retain_every_n_frames_in_test=1,
            with_damaged_actions=True,
            norm_image=True,
            crop=True,
            action=None,
            rectificated=True,
            train_subset=None,
            trans_to_meter = True,
            norm_2d = True,
            heatmaps_load = False,
            include_cameras=['60457274', '55011271'],):
        """
            h36m_root:
                Path to 'processed/' directory in Human3.6M
            labels_path:
                Path to 'human36m-multiview-labels.npy' generated by 'generate-labels-npy-multiview.py'
                from https://github.sec.samsung.net/RRU8-VIOLET/human36m-preprocessing
            retain_every_n_frames_in_test:
                By default, there are 159 181 frames in training set and 26 634 in test (val) set.
                With this parameter, test set frames will be evenly skipped frames so that the
                test set size is `26634 // retain_every_n_frames_test`.
                Use a value of 13 to get 2049 frames in test set.
            with_damaged_actions:
                If `True`, will include 'S9/[Greeting-2,SittingDown-2,Waiting-1]' in test set.
            kind:
                Keypoint format, 'mpii' or 'human36m'
            ignore_cameras:
                A list with indices of cameras to exclude (0 to 3 inclusive)
            dataset:
                Dataset, "h36m" or "TC"
        """
        assert train or test, '`Human36MMultiViewDataset` must be constructed with at least ' \
                              'one of `test=True` / `train=True`'

        self.joints_name = {
            0 : 'RFoot',
            1 : 'RKnee',
            2 : 'RHip',
            3 : 'LFoot',
            4 : 'LKnee',
            5 : 'LHip',
            6 : 'Spine',
            7 : 'Thorax',
            8 : 'Neck',
            9 : 'Head',
            10 : 'RShoulder',
            11 : 'RElbow',
            12 : 'RWrist',
            13 : 'LShoulder',
            14 : 'LElbow',
            15 : 'LWrist',
            16 : 'Nose'
        }

        self.h36m_root = h36m_root
        self.labels_path = labels_path
        self.image_shape = None if image_shape is None else tuple(image_shape)
        self.heatmap_shape = None if heatmap_shape is None else tuple(
            heatmap_shape)
        self.norm_image = norm_image
        self.crop = crop
        self.action_target = action
        self.rectificated=rectificated
        self.trans_to_meter = trans_to_meter
        self.norm_2d = norm_2d
        self.heatmaps_load = heatmaps_load
        self.include_cameras = include_cameras


        self.labels = np.load(labels_path, allow_pickle=True).item()
        
        train_subjects = ['S1', 'S5', 'S6', 'S7', 'S8']
        test_subjects = ['S9', 'S11']
        train_subjects = list(self.labels['subject_names'].index(x)
                                for x in train_subjects)
        test_subjects = list(self.labels['subject_names'].index(x)
                                for x in test_subjects)

        indices = []
        if train:
            mask = np.isin(self.labels['table']['subject_idx'],
                           train_subjects,
                           assume_unique=True)
            indices.append(np.nonzero(mask)[0])
        if test:
            mask = np.isin(self.labels['table']['subject_idx'],
                           test_subjects,
                           assume_unique=True)

            if not with_damaged_actions:
                mask_S9 = self.labels['table']['subject_idx'] == self.labels[
                    'subject_names'].index('S9')

                damaged_actions = 'Greeting-2', 'SittingDown-2', 'Waiting-1'
                damaged_actions = [
                    self.labels['action_names'].index(x)
                    for x in damaged_actions
                ]
                mask_damaged_actions = np.isin(
                    self.labels['table']['action_idx'], damaged_actions)

                mask &= ~(mask_S9 & mask_damaged_actions)

            indices.append(
                np.nonzero(mask)[0][::retain_every_n_frames_in_test])

        self.labels['table'] = self.labels['table'][np.concatenate(indices)]

        self.num_keypoints =  17

        
        self.keypoints_3d_pred = None
        if pred_results_path is not None:
            pred_results = np.load(pred_results_path, allow_pickle=True)
            if "keypoints_3d" in pred_results.keys():
                keypoints_3d_pred = pred_results['keypoints_3d'][np.argsort(
                    pred_results['indexes'])]
            elif "keypoints_3d_pred" in pred_results.keys():
                keypoints_3d_pred = pred_results['keypoints_3d_pred'][np.argsort(
                    pred_results['indexes'])]
            self.keypoints_3d_pred = keypoints_3d_pred[::
                                                       retain_every_n_frames_in_test]
            assert len(self.keypoints_3d_pred) == len(self), \
                f"[train={train}, test={test}] {labels_path} has {len(self)} samples, but '{pred_results_path}' " + \
                f"has {len(self.keypoints_3d_pred)}. Did you follow all preprocessing instructions carefully?"
        
        self.keypoints_2d_pred = None
        self.keypoints_2d_gt = None
        if pred_2d_results_path is not None:
            pred_2dresults = np.load(pred_2d_results_path, allow_pickle=True)
            keypoints_2d_pred = pred_2dresults['keypoints_2d_pred'][np.argsort(
                pred_2dresults['indexes'])]
            keypoints_2d_gt = pred_2dresults['keypoints_2d_gt'][np.argsort(
                pred_2dresults['indexes'])]
            self.keypoints_2d_pred = keypoints_2d_pred[::
                                                       retain_every_n_frames_in_test]
            self.keypoints_2d_gt = keypoints_2d_gt[::
                                                       retain_every_n_frames_in_test]            
            if self.norm_2d:
                self.keypoints_2d_pred = normalize_screen_coordinates(self.keypoints_2d_pred, self.image_shape[0],  self.image_shape[1])
                self.keypoints_2d_gt = normalize_screen_coordinates(self.keypoints_2d_gt, self.image_shape[0],  self.image_shape[1])
            assert len(self.keypoints_2d_pred) == len(self), \
                f"[train={train}, test={test}] {labels_path} has {len(self)} samples, but '{pred_results_path}' " + \
                f"has {len(self.keypoints_2d_pred)}. Did you follow all preprocessing instructions carefully?"
        
        self.pred_2d_kernel = None
        if pred_2d_error_dis_path is not None:
            self.pred_2d_kernel  = np.load(pred_2d_error_dis_path)

    def action_names_to_index_TC(self):
        return {
            'rom': 1,
            'walking': 2,
            'acting': 3,
            'running': 4,
            'freestyle': 5
        }

    def __len__(self):
        return len(self.labels['table'])

    def __getitem__(self, idx):
        sample = defaultdict(list)  # return value
        shot = self.labels['table'][idx]

        subject_idx = shot['subject_idx']
        action_idx = shot['action_idx']
        subject = self.labels['subject_names'][subject_idx]
        action = self.labels['action_names'][action_idx]
        frame_idx = shot['frame_idx']

        # mm to meter
        if self.trans_to_meter:
            shot['keypoints'] = shot['keypoints'] / 1000
        keypoints_3d = shot['keypoints'][:self.num_keypoints]
        keypoints_2d = []
        T_bino = []
        for camera_idx, camera_name in enumerate(self.labels['camera_names']):
            if camera_name not in self.include_cameras:
                continue
            # load bounding box
            bbox = shot['bbox_by_camera_tlbr'][camera_idx][[1, 0, 3,
                                                            2]]  # TLBR to LTRB
            
            bbox_height = bbox[2] - bbox[0]
            if bbox_height == 0:
                # convention: if the bbox is empty, then this view is missing
                continue

            # load camera
            shot_camera = self.labels['cameras'][subject_idx, camera_idx]
            T_ca = -(shot_camera['R'].T)@shot_camera['t']
            retval_camera = Camera(shot_camera['R'], T_ca,
                                   shot_camera['t'], shot_camera['K'],
                                   shot_camera['dist'], camera_name)
            T_bino.append(T_ca)

            if self.crop:
                # crop image
                retval_camera.update_after_crop(bbox)

            if self.image_shape is not None:
                # resize
                left, upper, right, lower = bbox
                image_shape_before_resize = (lower-upper, right-left)
                retval_camera.update_after_resize(image_shape_before_resize,
                                                  self.image_shape)
                sample['image_shapes_before_resize'].append(
                    image_shape_before_resize)
                
            # update camera parameters
            if self.trans_to_meter:
                retval_camera.trans_mm_to_meter()
            
            if self.norm_2d:
                retval_camera.updat_after_norm_2d(self.image_shape)
                keypoints_2d.append(multiview.project_3d_points_to_image_plane_without_distortion(retval_camera.projection, keypoints_3d))

            # TODO load 2d heatmaps

            sample['proj_matrices'].append(retval_camera.projection)
            sample['extrins_matrices'].append(retval_camera.extrinsics)
 
        sample['keypoints_2d'] = keypoints_2d

        sample['keypoints_3d'] = keypoints_3d
        sample['indexes'] = idx
        sample['action'] = action
        sample['action_idx'] = action_idx/11.0
        sample['baseline_width'] = np.sqrt(np.sum((T_bino[0]-T_bino[1])**2))

        if self.keypoints_3d_pred is not None:
            sample['pred_keypoints_3d'] = self.keypoints_3d_pred[idx]
        if self.keypoints_2d_pred is not None:
            sample['pred_keypoints_2d'] = self.keypoints_2d_pred[idx]
        if self.pred_2d_kernel is not None:
            sample['pred_2d_kernel'] = self.pred_2d_kernel

        sample.default_factory = None

        return sample
    

    def generate_target(self, joints, joints_vis= None):
        '''
        :param joints:  [num_joints, 3]
        :param joints_vis: [num_joints, 3]
        :return: target, target_weight(1: visible, 0: invisible)
        '''
        target_weight = np.ones((self.num_keypoints, 1), dtype=np.float32)
        if joints_vis is not None:
            target_weight[:, 0] = joints_vis[:, 0]

        assert self.target_type == 'gaussian', \
            'Only support gaussian map now!'

        if self.target_type == 'gaussian':
            target = np.zeros((self.num_keypoints,
                               self.heatmap_shape[1],
                               self.heatmap_shape[0]),
                              dtype=np.float32)

            tmp_size = self.sigma * 3

            for joint_id in range(self.num_keypoints):
                feat_stride = np.array(self.image_shape) / np.array(self.heatmap_shape)
                mu_x = int(joints[joint_id][0] / feat_stride[0] + 0.5)
                mu_y = int(joints[joint_id][1] / feat_stride[1] + 0.5)
                # Check that any part of the gaussian is in-bounds
                ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]
                br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]
                if ul[0] >= self.heatmap_shape[0] or ul[1] >= self.heatmap_shape[1] \
                        or br[0] < 0 or br[1] < 0:
                    # If not, just return the image as is
                    target_weight[joint_id] = 0
                    continue

                # # Generate gaussian
                size = 2 * tmp_size + 1
                x = np.arange(0, size, 1, np.float32)
                y = x[:, np.newaxis]
                x0 = y0 = size // 2
                # The gaussian is not normalized, we want the center value to equal 1
                g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * self.sigma ** 2))

                # Usable gaussian range
                g_x = max(0, -ul[0]), min(br[0], self.heatmap_shape[0]) - ul[0]
                g_y = max(0, -ul[1]), min(br[1], self.heatmap_shape[1]) - ul[1]
                # Image range
                img_x = max(0, ul[0]), min(br[0], self.heatmap_shape[0])
                img_y = max(0, ul[1]), min(br[1], self.heatmap_shape[1])

                v = target_weight[joint_id]
                if v > 0.5:
                    target[joint_id][img_y[0]:img_y[1], img_x[0]:img_x[1]] = \
                        g[g_y[0]:g_y[1], g_x[0]:g_x[1]]

        # if self.use_different_joints_weight:
        #     target_weight = np.multiply(target_weight, self.joints_weight)

        return target, target_weight

    def evaluate_using_per_pose_error(self, per_pose_error, split_by_subject):
        # print("pose_error.shape:", per_pose_error.shape)
        def evaluate_by_actions(self, per_pose_error, mask=None):
            if mask is None:
                mask = np.ones_like(per_pose_error, dtype=bool)

            action_scores = {
                'Average': {
                    'total_loss': per_pose_error[mask].sum(),
                    'frame_count': np.count_nonzero(mask)
                }
            }

            for action_idx in range(len(self.labels['action_names'])):
                action_mask = (self.labels['table']['action_idx']
                               == action_idx) & mask
                action_per_pose_error = per_pose_error[action_mask]
                action_scores[self.labels['action_names'][action_idx]] = {
                    'total_loss': action_per_pose_error.sum(),
                    'frame_count': len(action_per_pose_error)
                }

            action_names_without_trials = \
                [name[:-2] for name in self.labels['action_names'] if name.endswith('-1')]

            for action_name_without_trial in action_names_without_trials:
                combined_score = {'total_loss': 0.0, 'frame_count': 0}

                for trial in 1, 2:
                    action_name = '%s-%d' % (action_name_without_trial, trial)
                    combined_score['total_loss'] += action_scores[action_name][
                        'total_loss']
                    combined_score['frame_count'] += action_scores[
                        action_name]['frame_count']
                    del action_scores[action_name]

                action_scores[action_name_without_trial] = combined_score

            for k, v in action_scores.items():
                action_scores[k] = float('nan') if v['frame_count'] == 0 else (
                    v['total_loss'] / v['frame_count'])

            return action_scores

        subject_scores = {'Average': evaluate_by_actions(self, per_pose_error)}

        for subject_idx in range(len(self.labels['subject_names'])):
            subject_mask = self.labels['table']['subject_idx'] == subject_idx
            subject_scores[self.labels['subject_names'][subject_idx]] = \
                evaluate_by_actions(self, per_pose_error, subject_mask)

        return subject_scores

    def evaluate_TC(self,
                    keypoints_3d_predicted,
                    split_by_subject=False,
                    transfer_cmu_to_human36m=False,
                    transfer_human36m_to_human36m=False):
        keypoints_gt = self.labels['table']['keypoints'][:, :self.
                                                         num_keypoints]
        # mean error per 16/17 joints in mm, for each pose
        per_pose_error = np.sqrt(
            ((keypoints_gt - keypoints_3d_predicted)**2).sum(2)).mean(1)

        if not (transfer_cmu_to_human36m or transfer_human36m_to_human36m):
            root_index = 6 if self.kind == "mpii" else 6
        else:
            root_index = 0
        keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:
                                                            root_index + 1, :]
        keypoints_3d_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:,
                                                                                          root_index:
                                                                                          root_index
                                                                                          +
                                                                                          1, :]
        per_pose_error_relative = np.sqrt(
            ((keypoints_gt_relative -
              keypoints_3d_predicted_relative)**2).sum(2)).mean(1)

        result = {
            'per_pose_error':
            self.evaluate_using_per_pose_error(per_pose_error,
                                               split_by_subject),
            'per_pose_error_relative':
            self.evaluate_using_per_pose_error(per_pose_error_relative,
                                               split_by_subject)
        }

        return result['per_pose_error_relative']['Average']['Average'], result

    def evaluate(self,
                 keypoints_3d_predicted,
                 split_by_subject=False,
                 keypoints_3d_gt=None):
        root_index = 6
        if self.trans_to_meter:
            keypoints_3d_predicted = keypoints_3d_predicted * 1000
        
        if keypoints_3d_gt is None:
            keypoints_gt = self.labels['table']['keypoints'][:, :self.
                                                         num_keypoints]
        else:
            keypoints_gt = keypoints_3d_gt

        per_pose_error = np.sqrt(
            ((keypoints_gt - keypoints_3d_predicted)**2).sum(2)).mean(1)


        keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:
                                                            root_index + 1, :]
        keypoints_3d_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:,
                                                                                          root_index:
                                                                                          root_index
                                                                                          +
                                                                                          1, :]

        per_pose_error_relative = np.sqrt(
            ((keypoints_gt_relative -
              keypoints_3d_predicted_relative)**2).sum(2)).mean(1)

        if keypoints_3d_gt is None:
            result = {
                'per_pose_error':
                self.evaluate_using_per_pose_error(per_pose_error,
                                                split_by_subject),
                'per_pose_error_relative':
                self.evaluate_using_per_pose_error(per_pose_error_relative,
                                                split_by_subject)
            }
        else:
            result = {
            'per_pose_error':{'Average':{'Average':per_pose_error.mean()}},
            'per_pose_error_relative':{'Average':{'Average':per_pose_error_relative.mean()}}
        }

        return result['per_pose_error_relative']['Average'][
            'Average'], result['per_pose_error']['Average'][
            'Average'], result

    def evaluate_reasonbale(self,
                            keypoints_3d_predicted,
                            split_by_subject=False,
                            transfer_cmu_to_human36m=False,
                            transfer_human36m_to_human36m=False,
                            occupany_matrix=None):
        '''[evaluate the reasonability of pose]

        Args:
            keypoints_3d_predicted ([numpy(B, N, 3)]): [3d keypoints predicted]
            split_by_subject (bool, optional): [description]. Defaults to False.
            transfer_cmu_to_human36m (bool, optional): [description]. Defaults to False.
            transfer_human36m_to_human36m (bool, optional): [description]. Defaults to False.
            occupany_matrix ([numpy(K, 181, 361)], optional): [occupany matrix of joint_angle]. Defaults to None.

        Raises:
            ValueError: [description]

        Returns:
            [type]: [description]
        '''
        keypoints_gt = self.labels['table']['keypoints'][:, :self.
                                                         num_keypoints]
        if keypoints_3d_predicted.shape != keypoints_gt.shape:
            raise ValueError('`keypoints_3d_predicted` shape should be %s, got %s' % \
                (keypoints_gt.shape, keypoints_3d_predicted.shape))

        if transfer_cmu_to_human36m or transfer_human36m_to_human36m:
            human36m_joints = [10, 11, 15, 14, 1, 4]
            if transfer_human36m_to_human36m:
                cmu_joints = [10, 11, 15, 14, 1, 4]
            else:
                cmu_joints = [10, 8, 9, 7, 14, 13]

            keypoints_gt = keypoints_gt[:, human36m_joints]
            keypoints_3d_predicted = keypoints_3d_predicted[:, cmu_joints]

        # mean error per 16/17 joints in mm, for each pose
        per_pose_error = np.sqrt(
            ((keypoints_gt - keypoints_3d_predicted)**2).sum(2)).mean(1)
        # print(per_pose_error.shape)   # (samples, 1)
        # relative mean error per 16/17 joints in mm, for each pose
        if not (transfer_cmu_to_human36m or transfer_human36m_to_human36m):
            root_index = 6 if self.kind == "mpii" else 6
        else:
            root_index = 0

        keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:
                                                            root_index + 1, :]
        keypoints_3d_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:,
                                                                                          root_index:
                                                                                          root_index
                                                                                          +
                                                                                          1, :]

        # reasonable evaluate
        # bone length
        sample_nums = keypoints_3d_predicted.shape[0]
        bl_propor_reason_1 = np.zeros_like(per_pose_error)
        bl_propor_reason_2 = np.zeros_like(per_pose_error)
        bl_propor_reason_3 = np.zeros_like(per_pose_error)
        bl_propor_reason_4 = np.zeros_like(per_pose_error)
        bl_propor_reason_5 = np.zeros_like(per_pose_error)
        bl_bone_propor_reason_1 = np.zeros_like(per_pose_error)
        bl_bone_propor_reason_2 = np.zeros_like(per_pose_error)
        bl_bone_propor_reason_3 = np.zeros_like(per_pose_error)
        bl_bone_propor_reason_4 = np.zeros_like(per_pose_error)
        bl_bone_propor_reason_5 = np.zeros_like(per_pose_error)

        for sample_i in range(sample_nums):
            keypoint_gt = np.expand_dims(keypoints_gt_relative[sample_i],
                                         axis=0)
            keypoint_pred = np.expand_dims(
                keypoints_3d_predicted_relative[sample_i], axis=0)
            body_gt = body_kinematic.Body_Kinematic(keypoint_gt)
            body_pred = body_kinematic.Body_Kinematic(keypoint_pred)
            bl_gt = body_gt.keypoints2bone()
            bl_gt = body_gt.bone_length_mean(bl_gt)
            # print("bl_gt.shape:", bl_gt.shape)
            bl_res = body_pred.bl_eva(bl_gt, 0.1)
            bl_propor_reason_1[sample_i], bl_bone_propor_reason_1[
                sample_i] = bl_res[0][0], bl_res[1][0]
            bl_res = body_pred.bl_eva(bl_gt, 0.2)
            bl_propor_reason_2[sample_i], bl_bone_propor_reason_2[
                sample_i] = bl_res[0][0], bl_res[1][0]
            bl_res = body_pred.bl_eva(bl_gt, 0.3)
            bl_propor_reason_3[sample_i], bl_bone_propor_reason_2[
                sample_i] = bl_res[0][0], bl_res[1][0]
            bl_res = body_pred.bl_eva(bl_gt, 0.4)
            bl_propor_reason_4[sample_i], bl_bone_propor_reason_2[
                sample_i] = bl_res[0][0], bl_res[1][0]
            bl_res = body_pred.bl_eva(bl_gt, 0.5)
            bl_propor_reason_5[sample_i], bl_bone_propor_reason_2[
                sample_i] = bl_res[0][0], bl_res[1][0]
        # print("bl_propor_reason_1.shape:", bl_propor_reason_1.shape)

        #joint angle
        body_pred = body_kinematic.Body_Kinematic(
            keypoints_3d_predicted_relative)
        # occupany_matrix = np.load("./resnb_data/occupany_matrix.npy")
        ja_res = body_pred.angle_eva(occupany_matrix)
        joint_angle_reason, joint_angle_pjoint_reason = ja_res[0].squeeze(
            1), ja_res[1].squeeze(1)
        # print("joint_angle_reason.shape:", joint_angle_reason.shape)

        #all
        all_reason_1 = np.zeros_like(per_pose_error)
        all_reason_2 = np.zeros_like(per_pose_error)
        all_reason_3 = np.zeros_like(per_pose_error)
        all_reason_4 = np.zeros_like(per_pose_error)
        all_reason_5 = np.zeros_like(per_pose_error)
        for i, x in enumerate(joint_angle_reason):
            if (x == 1 and bl_propor_reason_1[i] == 1):
                all_reason_1[i] = 1
            if (x == 1 and bl_propor_reason_2[i] == 1):
                all_reason_2[i] = 1
            if (x == 1 and bl_propor_reason_3[i] == 1):
                all_reason_3[i] = 1
            if (x == 1 and bl_propor_reason_4[i] == 1):
                all_reason_4[i] = 1
            if (x == 1 and bl_propor_reason_5[i] == 1):
                all_reason_5[i] = 1
        # print("all_reason_1.shape:", all_reason_1.shape)

        result = {
            'reasonable_1':
            self.evaluate_using_per_pose_error(all_reason_1, split_by_subject),
            'reasonable_2':
            self.evaluate_using_per_pose_error(all_reason_2, split_by_subject),
            'reasonable_3':
            self.evaluate_using_per_pose_error(all_reason_3, split_by_subject),
            'reasonable_4':
            self.evaluate_using_per_pose_error(all_reason_4, split_by_subject),
            'reasonable_5':
            self.evaluate_using_per_pose_error(all_reason_5, split_by_subject),
            'bl_reasonable_1':
            self.evaluate_using_per_pose_error(bl_propor_reason_1,
                                               split_by_subject),
            'bl_reasonable_2':
            self.evaluate_using_per_pose_error(bl_propor_reason_2,
                                               split_by_subject),
            'bl_bone_reasonable_1':
            self.evaluate_using_per_pose_error(bl_bone_propor_reason_1,
                                               split_by_subject),
            'bl_bone_reasonable_2':
            self.evaluate_using_per_pose_error(bl_bone_propor_reason_2,
                                               split_by_subject),
            'joint_angle_reason':
            self.evaluate_using_per_pose_error(joint_angle_reason,
                                               split_by_subject),
            'joint_angle_pjoint_reason':
            self.evaluate_using_per_pose_error(joint_angle_pjoint_reason,
                                               split_by_subject)
        }

        return result['reasonable_2']['Average'][
            'Average'], result, bl_propor_reason_2, joint_angle_reason, all_reason_2

    def evaluate_split_by_joints(self,
                                 keypoints_3d_predicted,
                                 split_by_subject=False,
                                 transfer_cmu_to_human36m=False,
                                 transfer_human36m_to_human36m=False,
                                 keypoints_gt=None):
        if keypoints_gt is None:
            keypoints_gt = self.labels['table']['keypoints'][:, :self.
                                                             num_keypoints]
        if keypoints_3d_predicted.shape != keypoints_gt.shape:
            raise ValueError(
                '`keypoints_3d_predicted` shape should be %s, got %s' % \
                (keypoints_gt.shape, keypoints_3d_predicted.shape))

        if transfer_cmu_to_human36m or transfer_human36m_to_human36m:
            human36m_joints = [10, 11, 15, 14, 1, 4]
            if transfer_human36m_to_human36m:
                cmu_joints = [10, 11, 15, 14, 1, 4]
            else:
                cmu_joints = [10, 8, 9, 7, 14, 13]

            keypoints_gt = keypoints_gt[:, human36m_joints]
            keypoints_3d_predicted = keypoints_3d_predicted[:, cmu_joints]

        # mean error each joint in mm, for each pose
        per_pose_error = np.sqrt(
            ((keypoints_gt - keypoints_3d_predicted)**2).sum(2))
        # print("per_pose_error:", per_pose_error.shape)
        # print(per_pose_error.shape)   # (samples, joints, 1)
        # relative mean error per 16/17 joints in mm, for each pose
        if not (transfer_cmu_to_human36m or transfer_human36m_to_human36m):
            root_index = 6 if self.kind == "mpii" else 6
        else:
            root_index = 0

        keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:
                                                            root_index + 1, :]
        keypoints_3d_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:,
                                                                                          root_index:
                                                                                          root_index
                                                                                          +
                                                                                          1, :]

        per_pose_error_relative = np.sqrt(
            ((keypoints_gt_relative -
              keypoints_3d_predicted_relative)**2).sum(2))
        # print("per_pose_error_relative:", per_pose_error_relative.shape)

        # bone length mean error per 16 bone in mm for each pose
        skeleton_idx = [[6, 3], [6, 2], [2, 1], [3, 4], [1, 0], [4, 5], [6, 7],
                        [7, 8], [8, 9], [8, 12], [8, 13], [12, 11], [11, 10],
                        [13, 14], [14, 15]]
        trans_matric = np.zeros((17, len(skeleton_idx)))
        for g in range(len(skeleton_idx)):
            id1, id2 = skeleton_idx[g]
            trans_matric[id1, g] = 1
            trans_matric[id2, g] = -1
        bone_gt_x = keypoints_gt[:, :, 0] @ trans_matric
        bone_gt_y = keypoints_gt[:, :, 1] @ trans_matric
        bone_gt_z = keypoints_gt[:, :, 2] @ trans_matric
        bone_pred_x = keypoints_3d_predicted[:, :, 0] @ trans_matric
        bone_pred_y = keypoints_3d_predicted[:, :, 1] @ trans_matric
        bone_pred_z = keypoints_3d_predicted[:, :, 2] @ trans_matric
        # print("bone_gt_x.shape:", bone_gt_x.shape) #(smaples, bonenums)
        # KCS矩阵
        per_pose_bonelength_error = np.zeros_like(per_pose_error)
        for s in range(bone_gt_x.shape[0]):
            # print(s)
            bone_length_gt_x = bone_gt_x[s].reshape(
                -1, 1) @ bone_gt_x[s].reshape(-1, 1).T
            bone_length_gt_y = bone_gt_y[s].reshape(
                -1, 1) @ bone_gt_y[s].reshape(-1, 1).T
            bone_length_gt_z = bone_gt_z[s].reshape(
                -1, 1) @ bone_gt_z[s].reshape(-1, 1).T
            bone_length_pred_x = bone_pred_x[s].reshape(
                -1, 1) @ bone_pred_x[s].reshape(-1, 1).T
            bone_length_pred_y = bone_pred_y[s].reshape(
                -1, 1) @ bone_pred_y[s].reshape(-1, 1).T
            bone_length_pred_z = bone_pred_z[s].reshape(
                -1, 1) @ bone_pred_z[s].reshape(-1, 1).T
            # print("bone_length_gt_x:", bone_length_gt_x.shape) #(bonenums, bonenums)
            bone_error = 0
            for i in range(bone_length_gt_x.shape[0]):
                bone_length_gt = np.sqrt(bone_length_gt_x[i, i] +
                                         bone_length_gt_y[i, i] +
                                         bone_length_gt_z[i, i])
                bone_length_pred = np.sqrt(bone_length_pred_x[i, i] +
                                           bone_length_pred_y[i, i] +
                                           bone_length_pred_z[i, i])
                # print("bonelength:", bone_length_gt, bone_length_pred)
                bone_error = abs(bone_length_gt - bone_length_pred)
                per_pose_bonelength_error[s, i] = bone_error
        # print("per_pose_bonelength_error:", per_pose_bonelength_error.shape)

        n_joints = keypoints_gt.shape[-2]
        result = dict()
        result_relative = dict()
        for n_joint in range(n_joints):
            result[n_joint] = self.evaluate_using_per_pose_error(
                per_pose_error[:, n_joint], split_by_subject)
            result_relative[n_joint] = self.evaluate_using_per_pose_error(
                per_pose_error_relative[:, n_joint], split_by_subject)
        # print("result:", result)
        # print("result_re:", result_relative)

        result_bonelength = dict()
        for n_joint in range(n_joints):
            result_bonelength[n_joint] = self.evaluate_using_per_pose_error(
                per_pose_bonelength_error[:, n_joint], split_by_subject)
        # print("result_bone:", result_bonelength)

        return result, result_relative, result_bonelength
    
