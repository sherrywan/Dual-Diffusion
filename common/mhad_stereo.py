
import os
from collections import defaultdict
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset

from common.camera import Camera, normalize_screen_coordinates, unnormalize_screen_coordinates
from common.img import crop_keypoints_img, resize_keypoints_img
from common import multiview

class MHADStereoViewDataset(Dataset):
    """
        MHAD_Berkeley for stereoview tasks
    """
    def __init__(
            self,
            mhad_root='/data1/share/dataset/MHAD_Berkeley/stereo_camera',
            labels_path='/data1/share/dataset/MHAD_Berkeley/stereo_camera/extra/mhad-stereo-s-labels-GTbboxes-viewsharing.npy',
            pred_results_path=None,
            pred_2d_results_path=None,
            pred_2d_error_dis_path=None,
            image_shape=(256, 256),
            heatmap_shape=(64, 64),
            train=False,
            test=False,
            retain_every_n_frames_in_test=1,
            norm_image=True,
            crop=True,
            action=None,
            rectificated=True,
            baseline_width='s',
            train_subset=None,
            trans_to_meter = True,
            norm_2d = True,
            heatmaps_load = False,
            flip=False):
        """
            mhad_root:
                Path to 'stereo_camera/' directory in MHAD
            labels_path:
                Path to 'human36m-stereo-s-labels-GTbboxes.npy' generated by 'generate-stereo-labels.py'
            retain_every_n_frames_in_test:
            kind:
                Keypoint format, 'mpii' or 'mhad'
            dataset:
                Dataset, "mhad"
            rectificated:
                If the stereo images are rectificated
            baseline_width:
                Type of baseline width of stereo cameras
        """
        assert train or test, '`MHADStereoViewDataset` must be constructed with at least ' \
                              'one of `test=True` / `train=True`'
        self.joints_name = {
            0 : 'RFoot',
            1 : 'RKnee',
            2 : 'RHip',
            3 : 'LHip',
            4 : 'LKnee',
            5 : 'LFoot',
            6 : 'Spine',
            7 : 'Thorax',
            8 : 'Neck',
            9 : 'Head',
            10 : 'RWrist',
            11 : 'RElbow',
            12 : 'RShoulder',
            13 : 'LShoulder',
            14 : 'LElbow',
            15 : 'LWrist',
            16 : 'Nose'
        }
        self.mhad_root = mhad_root
        self.labels_path = labels_path
        self.is_train = train
        self.image_shape = None if image_shape is None else tuple(image_shape)
        self.heatmap_shape = None if heatmap_shape is None else tuple(
            heatmap_shape)
        self.norm_image = norm_image
        self.crop = crop
        self.action_target = action
        self.rectificated = rectificated
        self.baseline_width = baseline_width
        self.trans_to_meter = trans_to_meter
        self.norm_2d = norm_2d
        self.heatmaps_load = heatmaps_load
        self.flip=flip

        self.labels = np.load(labels_path, allow_pickle=True).item()
        
        train_subjects = ['S01','S02','S03','S04','S05','S06','S07','S09','S10','S12']
        if train_subset is not None:
            train_subjects = train_subset
        test_subjects = ['S08', 'S11']
        train_subjects = list(self.labels['subject_names'].index(x)
                                for x in train_subjects)
        test_subjects = list(self.labels['subject_names'].index(x)
                                for x in test_subjects)

        indices = []
        if train:
            mask = np.isin(self.labels['table']['subject_idx'],
                           train_subjects,
                           assume_unique=True)
            indices.append(np.nonzero(mask)[0])
        if test:
            mask = np.isin(self.labels['table']['subject_idx'],
                           test_subjects,
                           assume_unique=True)
            indices.append(np.nonzero(mask)[0][::retain_every_n_frames_in_test])

        self.labels['table'] = self.labels['table'][np.concatenate(indices)]

        self.num_keypoints = 17

        self.keypoints_3d_pred = None
        self.keypoints_2d_pred = None
        if pred_results_path is not None:
            pred_results = np.load(pred_results_path, allow_pickle=True)
            if "big" in pred_results_path or "small" in pred_results_path:
                indices=pred_results['indexes']
                self.labels['table'] = self.labels['table'][indices]
            keypoints_3d_pred = pred_results['keypoints_3d'][np.argsort(
                pred_results['indexes'])]
            self.keypoints_3d_pred = keypoints_3d_pred[::
                                                       retain_every_n_frames_in_test]
            if train_subset is None:
                assert len(self.keypoints_3d_pred) == len(self), \
                    f"[train={train}, test={test}] {labels_path} has {len(self)} samples, but '{pred_results_path}' " + \
                    f"has {len(self.keypoints_3d_pred)}. Did you follow all preprocessing instructions carefully?"
            if self.trans_to_meter:
                self.keypoints_3d_pred = self.keypoints_3d_pred / 1000
            if 'keypoints_2d_pred' in pred_results.keys():
                keypoints_2d_pred = pred_results['keypoints_2d_pred'][np.argsort(
                pred_results['indexes'])]
                self.keypoints_2d_pred = keypoints_2d_pred[::
                                                        retain_every_n_frames_in_test]
                if self.norm_2d:
                    self.keypoints_2d_pred = normalize_screen_coordinates(self.keypoints_2d_pred, self.image_shape[0],  self.image_shape[1])
                if train_subset is None:
                    assert len(self.keypoints_2d_pred) == len(self), \
                        f"[train={train}, test={test}] {labels_path} has {len(self)} samples, but '{pred_results_path}' " + \
                        f"has {len(self.keypoints_2d_pred)}. Did you follow all preprocessing instructions carefully?"

        if pred_2d_results_path is not None:
            # group=0 for adafuse and epipolar-Tri
            if "adafuse" in pred_2d_results_path or "epi" in pred_2d_results_path:
                indices=[]
                mask = np.isin(self.labels['table']['group_idx'],
                                1,
                                assume_unique=True)
                indices.append(np.nonzero(mask)[0])
                self.labels['table'] = self.labels['table'][np.concatenate(indices)]
            pred_2d_error_dis_path
            pred_2dresults = np.load(pred_2d_results_path, allow_pickle=True)
            keypoints_2d_pred = pred_2dresults['keypoints_2d_pred'][np.argsort(
                pred_2dresults['indexes'])]
            self.keypoints_2d_pred = keypoints_2d_pred[::
                                                       retain_every_n_frames_in_test]
            if self.norm_2d:
                self.keypoints_2d_pred = normalize_screen_coordinates(self.keypoints_2d_pred, self.image_shape[0],  self.image_shape[1])
            if train_subset is None:
                assert len(self.keypoints_2d_pred) == len(self), \
                    f"[train={train}, test={test}] {labels_path} has {len(self)} samples, but '{pred_results_path}' " + \
                    f"has {len(self.keypoints_2d_pred)}. Did you follow all preprocessing instructions carefully?"
        
        self.pred_2d_kernel = None
        if pred_2d_error_dis_path is not None:
            self.pred_2d_kernel  = np.load(pred_2d_error_dis_path)

        
    def __len__(self):
        return len(self.labels['table'])


    def __getitem__(self, idx):
        sample = defaultdict(list)  # return value
        shot = self.labels['table'][idx]

        group_idx = shot['group_idx']
        subject_idx = shot['subject_idx']
        action_idx = shot['action_idx']
        reptition_idx = shot['reptition_idx']
        subject = self.labels['subject_names'][subject_idx]
        action = self.labels['action_names'][action_idx]
        reptition = self.labels['reptition_names'][reptition_idx]
        frame_idx = shot['frame_idx']
        
        if self.action_target is not None and self.action_target not in action:
            return None
    
        # mm to meter
        if self.trans_to_meter:
            shot['keypoints'] = shot['keypoints'] / 1000
        keypoints_3d = shot['keypoints'][:self.num_keypoints]
        if self.flip:
            keypoints_3d_re = keypoints_3d - keypoints_3d[6:7,:]
            keypoints_3d_re[:,0] = -keypoints_3d_re[:,0]
            keypoints_3d = keypoints_3d_re + keypoints_3d[6:7,:]

        T_bino = []
        for camera_idx, camera_name in enumerate(self.labels['camera_names'][group_idx]):
            # load bounding box
            bbox = shot['bbox_by_camera_tlbr'][camera_idx][[1, 0, 3, 2]]  # TLBR to LTRB
            bbox_height = bbox[2] - bbox[0]
            if bbox_height == 0:
                # convention: if the bbox is empty, then this view is missing
                continue

            # load camera
            shot_camera = self.labels['cameras'][group_idx][camera_idx]
            T = -shot_camera['R'].T @ shot_camera['t']
            retval_camera = Camera(shot_camera['R'], T,
                                   shot_camera['t'], shot_camera['K'],
                                   name = camera_name)
            T_bino.append(T)
            # load keypoints_2d
            keypoints_2d = shot['keypoints_2d'][:self.num_keypoints][camera_idx]

            if self.crop:
                # crop image
                retval_camera.update_after_crop(bbox)
                keypoints_2d = crop_keypoints_img(keypoints_2d, bbox)

            if self.image_shape is not None:
                # resize
                left, upper, right, lower = bbox
                image_shape_before_resize = (lower-upper, right-left)
                retval_camera.update_after_resize(image_shape_before_resize,
                                                  self.image_shape)
                sample['image_shapes_before_resize'].append(
                    image_shape_before_resize)
                keypoints_2d = resize_keypoints_img(keypoints_2d, image_shape_before_resize, self.image_shape)
            
            # update camera parameters
            if self.trans_to_meter:
                retval_camera.trans_mm_to_meter()
            
            if self.norm_2d:
                keypoints_2d_norm = normalize_screen_coordinates(keypoints_2d[:,:-1], self.image_shape[0],  self.image_shape[1])
                retval_camera.updat_after_norm_2d(self.image_shape)
                keypoints_2d_norm=multiview.project_3d_points_to_image_plane_without_distortion(retval_camera.projection, keypoints_3d)

            # load 2d heatmaps
            if self.heatmaps_load:
                home_path = "Cluster01"
                home_path = "Mid_Res"
                
                heatmap_path = os.path.join(
                        self.mhad_root, home_path, 'rectificated' * self.rectificated, 
                    self.baseline_width * self.rectificated, str(self.heatmap_shape[0]), str(group_idx) * self.rectificated, 
                    'Cam%02d'%(int(camera_name)), subject, action, reptition, 
                    'heatmap_l01_c%02d_s%02d_a%02d_r%02d_%05d.npy' % (int(camera_name), subject_idx+1, action_idx+1, reptition_idx+1, frame_idx))
                
                if os.path.isfile(heatmap_path):
                    heatmaps_2d = np.load(heatmap_path)
                    assert heatmaps_2d.shape[-1]==self.heatmap_shape[-1], "make sure that heatmaps shape \"{}\" are same with {}!".format(heatmap_path, self.heatmap_shape[-1])
                    sample['heatmaps'].append(heatmaps_2d)
                else:
                    heatmaps_2d, heatmaps_weight = self.generate_target(keypoints_2d, self.heatmap_shape[0]/32)
                    heatmap_folder = os.path.join(
                        self.mhad_root, home_path, 'rectificated' * self.rectificated, 
                    self.baseline_width * self.rectificated, str(self.heatmap_shape[0]), str(group_idx) * self.rectificated, 
                    'Cam%02d'%(int(camera_name)), subject, action, reptition,)
                    os.makedirs(heatmap_folder, exist_ok=True)
                    np.save(heatmap_path, heatmaps_2d)
                    sample['heatmaps'].append(heatmaps_2d)
            
            sample['proj_matrices'].append(retval_camera.projection)
            sample['extrins_matrices'].append(retval_camera.extrinsics)
            sample['keypoints_2d'].append(keypoints_2d_norm)

        sample['keypoints_3d'] = keypoints_3d
        sample['indexes'] = idx
        sample['action'] = action
        sample['action_idx'] = action_idx/11.0
        sample['baseline_width'] = np.sqrt(np.sum((T_bino[0]-T_bino[1])**2))

        if self.keypoints_3d_pred is not None:
            sample['pred_keypoints_3d'] = self.keypoints_3d_pred[idx]
        if self.keypoints_2d_pred is not None:
            sample['pred_keypoints_2d'] = self.keypoints_2d_pred[idx]
        if self.pred_2d_kernel is not None:
            sample['pred_2d_kernel'] = self.pred_2d_kernel

        sample.default_factory = None

        return sample

    def get_kinematic_params(self):
        return self.limb, self.kin_rotation, self.word_axis, self.layer_nums
    
    def get_kinematic_matrix(self):
        return self.kcs_m, self.kin_ro_m, self.kin_ro_re_m
    
    def generate_target(self, joints, sigma, joints_vis=None):
        '''
        :param joints:  [num_joints, 3]
        :param joints_vis: [num_joints, 3]
        :return: target, target_weight(1: visible, 0: invisible)
        '''
        
        target_weight = np.ones((self.num_keypoints, 1), dtype=np.float32)
        if joints_vis is not None:
            target_weight[:, 0] = joints_vis[:, 0]

        # assert self.target_type == 'gaussian', \
        #     'Only support gaussian map now!'

        # if self.target_type == 'gaussian':
        target = np.zeros((self.num_keypoints,
                            self.heatmap_shape[1],
                            self.heatmap_shape[0]),
                            dtype=np.float32)
        target_width = np.tile(np.arange(self.heatmap_shape[0]), [self.heatmap_shape[1], 1])
        target_height = np.tile(np.arange(self.heatmap_shape[1]).reshape(-1,1), [1, self.heatmap_shape[0]])
        target_loc = np.stack((target_height, target_width),axis=-1)

        tmp_size = sigma * 3

        for joint_id in range(self.num_keypoints):
            image_shape = np.array(self.image_shape)
            heatmap_shape = np.array(self.heatmap_shape)
            feat_stride = image_shape / heatmap_shape
            mu_x = joints[joint_id][0] / feat_stride[0]
            mu_y = joints[joint_id][1] / feat_stride[1]
            # check center of gaussion is in-bounds
            if mu_x >= self.heatmap_shape[0] or mu_y >= self.heatmap_shape[1] \
                    or mu_x < 0 or mu_y < 0:
                # If not, just return the image as is
                target_weight[joint_id] = 0
                continue
    
            v = target_weight[joint_id]

            if v > 0.5:
                for locs in target_loc:
                    for loc in locs:
                        target[joint_id][loc[0], loc[1]] = np.exp(- ((loc[1] - mu_x) ** 2 + (loc[0] - mu_y) ** 2) / (2 * sigma ** 2))
                    # normalize
                    # target[joint_id] = target[joint_id] / np.max(target[joint_id])

        # if self.use_different_joints_weight:
        #     target_weight = np.multiply(target_weight, self.num_keypoints)

        return target, target_weight


    def evaluate_using_per_pose_error(self, per_pose_error, split_by_subject):
        def evaluate_by_actions(self, per_pose_error, mask=None):
            if mask is None:
                mask = np.ones_like(per_pose_error, dtype=bool)

            action_scores = {
                'Average': {
                    'total_loss': per_pose_error[mask].sum(),
                    'frame_count': np.count_nonzero(mask)
                }
            }

            for action_idx in range(len(self.labels['action_names'])):
                action_mask = (self.labels['table']['action_idx']
                               == action_idx) & mask
                action_per_pose_error = per_pose_error[action_mask]
                action_scores[self.labels['action_names'][action_idx]] = {
                    'total_loss': action_per_pose_error.sum(),
                    'frame_count': len(action_per_pose_error)
                }

            for k, v in action_scores.items():
                action_scores[k] = float('nan') if v['frame_count'] == 0 else (
                    v['total_loss'] / v['frame_count'])

            return action_scores

        subject_scores = {'Average': evaluate_by_actions(self, per_pose_error)}

        for subject_idx in range(len(self.labels['subject_names'])):
            subject_mask = self.labels['table']['subject_idx'] == subject_idx
            subject_scores[self.labels['subject_names'][subject_idx]] = \
                evaluate_by_actions(self, per_pose_error, subject_mask)

        return subject_scores


    def evaluate(self,
                 keypoints_3d_predicted,
                 split_by_subject=False,
                 keypoints_3d_gt=None):
        '''
        occluded: if the keypoints are occluded, 0 for not occluded, 1 for oclluded, 2 for all
        '''
        # for ada ppt epitri
        keypoints_gt = self.labels['table']['keypoints'][:, :self.num_keypoints]        
        if keypoints_3d_predicted.shape != keypoints_gt.shape:
            self.labels['table'] = self.labels['table'][:keypoints_3d_predicted.shape[0]]
        root_index = 6
        occlusion = self.labels['table']['occlusion_2d'][:, :self.num_keypoints]
        if keypoints_3d_gt is None:
            keypoints_gt = self.labels['table']['keypoints'][:, :self.num_keypoints]   
        else:
            keypoints_gt = keypoints_3d_gt     
        
        if self.trans_to_meter:
            keypoints_3d_predicted = keypoints_3d_predicted * 1000
            # keypoints_gt = keypoints_gt * 1000

        keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:root_index + 1, :]
        keypoints_3d_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:,root_index:root_index+1, :]
        
        if keypoints_3d_predicted.shape != keypoints_gt.shape:
            raise ValueError('`keypoints_3d_predicted` shape should be %s, got %s' % \
                (keypoints_gt.shape, keypoints_3d_predicted.shape))
        
        # not occluded keypoints
        not_occluded = np.nonzero(occlusion==0)
        keypoints_gt_notocc = np.expand_dims(keypoints_gt[not_occluded[0], not_occluded[1],:], axis=0)
        keypoints_3d_predicted_notocc = np.expand_dims(keypoints_3d_predicted[not_occluded[0], not_occluded[1], :], axis=0)
        keypoints_gt_relative_notocc = np.expand_dims(keypoints_gt_relative[not_occluded[0], not_occluded[1], :], axis=0)
        keypoints_3d_predicted_relative_notocc = np.expand_dims(keypoints_3d_predicted_relative[not_occluded[0], not_occluded[1], :], axis=0)
        
        # occluded keypoints
        occluded = np.nonzero(occlusion==1)
        keypoints_gt_occ = np.expand_dims(keypoints_gt[occluded[0], occluded[1],:], axis=0)
        keypoints_3d_predicted_occ = np.expand_dims(keypoints_3d_predicted[occluded[0], occluded[1], :], axis=0)
        keypoints_gt_relative_occ = np.expand_dims(keypoints_gt_relative[occluded[0], occluded[1], :], axis=0)
        keypoints_3d_predicted_relative_occ = np.expand_dims(keypoints_3d_predicted_relative[occluded[0], occluded[1], :], axis=0)
        
        # mean error per 16/17 joints in mm, for each pose
        per_pose_error = np.sqrt(((keypoints_gt - keypoints_3d_predicted)**2).sum(2)).mean(1)  
        per_joint_error = np.sqrt(((keypoints_gt - keypoints_3d_predicted)**2).sum(2)).mean(0)
        per_pose_error_notocc = np.sqrt(((keypoints_gt_notocc - keypoints_3d_predicted_notocc)**2).sum(2)).mean(1)
        per_pose_error_occ = np.sqrt(((keypoints_gt_occ - keypoints_3d_predicted_occ)**2).sum(2)).mean(1)
        per_pose_error_relative_notocc = np.sqrt(((keypoints_gt_relative_notocc - keypoints_3d_predicted_relative_notocc)**2).sum(2)).mean(1)
        per_pose_error_relative_occ = np.sqrt(((keypoints_gt_relative_occ - keypoints_3d_predicted_relative_occ)**2).sum(2)).mean(1)
       
        # relative mean error per 16/17 joints in mm, for each pose
        per_pose_error_relative = np.sqrt(((keypoints_gt_relative - keypoints_3d_predicted_relative)**2).sum(2)).mean(1)
        
        
        result = {
            'per_pose_error':
            self.evaluate_using_per_pose_error(per_pose_error,
                                               split_by_subject),
            'per_pose_error_relative':
            self.evaluate_using_per_pose_error(per_pose_error_relative,
                                               split_by_subject),
            'per_joint_error':
            {'Average': per_joint_error.tolist()},
            'per_error_notocc':
            {'Average': per_pose_error_notocc.tolist()},
            'per_error_occ':
            {'Average': per_pose_error_occ.tolist()},
            'per_error_relative_notocc':
            {'Average': per_pose_error_relative_notocc.tolist()},
            'per_error_relative_occ':
            {'Average': per_pose_error_relative_occ.tolist()}
        }

        return result['per_pose_error_relative']['Average'][
            'Average'], result['per_pose_error']['Average'][
            'Average'], result
    
